{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Dependencies\n",
        "\n",
        "The following code cell will use `pip` to install all the necessary dependencies listed in the `requirements.txt` file. This ensures that all the required packages are available for the project to run correctly."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Description\n",
        "\n",
        "The following code snippet demonstrates how to configure and initialize the Azure OpenAI client for extracting text from PDFs using PyMuPDF (`fitz`).\n",
        "\n",
        "1. **Importing Libraries**:\n",
        "   - `fitz`: PyMuPDF library for extracting text from PDF files.\n",
        "   - `AzureOpenAI`: Library for interacting with Azure OpenAI services.\n",
        "   - `json` and `re`: Standard Python libraries for handling JSON data and regular expressions, respectively.\n",
        "\n",
        "2. **Azure OpenAI Configuration**:\n",
        "   - `AZURE_OPENAI_API_KEY`: Your Azure OpenAI API key.\n",
        "   - `AZURE_OPENAI_ENDPOINT`: Your Azure OpenAI endpoint URL.\n",
        "   - `DEPLOYMENT_NAME`: The name of your Azure OpenAI deployment.\n",
        "   - `AZURE_OPENAI_VERSION`: The version of the Azure OpenAI API you are using.\n",
        "\n",
        "3. **Initializing the Azure OpenAI Client**:\n",
        "   - The `AzureOpenAI` client is initialized with the provided API key, API version, and endpoint URL."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF for extracting text from PDF\n",
        "from openai import AzureOpenAI\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Azure OpenAI Configuration\n",
        "AZURE_OPENAI_API_KEY = \"your_azure_openai_api_key\"\n",
        "AZURE_OPENAI_ENDPOINT = \"your_azure_openai_endpoint\"\n",
        "DEPLOYMENT_NAME = \"your_deployment_name\" \n",
        "AZURE_OPENAI_VERSION = \"your_openai_version\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        " api_key=AZURE_OPENAI_API_KEY,\n",
        " api_version=AZURE_OPENAI_VERSION,\n",
        " azure_endpoint=AZURE_OPENAI_ENDPOINT )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1739997935859
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Text from PDF and Generating Synthetic Data\n",
        "\n",
        "The following code snippet demonstrates how to extract text from a PDF file, clean JSON responses, generate synthetic data using Azure OpenAI, and save the data to a JSONL file."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\") + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "def clean_json_response(response_text):\n",
        "    response_text = response_text.strip()\n",
        "    response_text = re.sub(r\"^```jsonl\\s*\", \"\", response_text) # Remove Leading ```json\n",
        "    response_text = re.sub(r\"```$\", \"\", response_text) # Remove trailing ```\n",
        "    return response_text.strip()\n",
        "\n",
        "def generate_synthetic_data(text, num_samples=2):\n",
        "    \"\"\"Generate synthetic dataset using Azure OpenAI.\"\"\"\n",
        "    synthetic_data = []\n",
        "    \n",
        "    \n",
        "    for _ in range(num_samples):\n",
        "        prompt = (\n",
        "        \"Generate a structured JSONL dataset for model evaluation where the <user prompt> is a sentence \"\n",
        "        \"and <expected completion> is the continuation of that sentence. The format should be: \"\n",
        "        \"{'input': [{'role': 'system', 'content': '<system message>'}, {'role': 'user', 'content': '<user prompt>'}], \"\n",
        "        \"'output': '<expected completion>'}. Ensure the output is valid JSONL.\\n\\n\"\n",
        "        \"everything should have semantic similarity like this {'input': [{'role': 'system', 'content': 'Provide a clear and concise summary of the technical content, highlighting key concepts and their relationships. Focus on the main ideas and practical implications.'}, {'role': 'user', 'content': 'Tokenization is a key step in preprocessing for natural language processing, involving the division of text into smaller components called tokens. These can be words, subwords, or characters, depending on the method chosen. Word tokenization divides text at word boundaries, while subword techniques like Byte Pair Encoding (BPE) or WordPiece can manage unknown words by breaking them into subunits. Character tokenization splits text into individual characters, useful for multiple languages and misspellings. The tokenization method chosen greatly affects model performance and its capacity to handle various languages and vocabularies.'}], 'output': 'Tokenization divides text into smaller units (tokens) for NLP applications, using word, subword (e.g., BPE), or character methods. Each has unique benefits, impacting model performance and language processing capabilities.'}  \"    \n",
        "        \"{'input': [{'role': 'system', 'content': 'Create a comprehensive yet concise summary that captures the essential technical details and significance of the topic.'}, {'role': 'user', 'content': 'Self-attention mechanisms are vital in modern transformer models, allowing them to evaluate the relevance of different parts of an input sequence when processing each element. In self-attention, each position in a sequence learns to focus on all other positions, forming a weighted sum of their values. Attention weights are derived using queries, keys, and values, with compatibility between a query and key determining attention to each value. This mechanism enables capturing long-range dependencies and relationships within a sequence, making it effective for tasks needing context understanding and inter-part relationships.'}], 'output': 'Self-attention is crucial in transformers, enabling each sequence position to evaluate and collect data from all others. It uses queries, keys, and values for attention weights, effectively capturing long-range dependencies and contextual links.'}  \"\n",
        "        f\"Generate a structured synthetic dataset based on this text  with semantic similarity between the <system message> and <expected completion>:\\n{text}\"\n",
        "    )\n",
        "\n",
        "        # Create the chat completion\n",
        "        response = client.chat.completions.create(\n",
        "            model=DEPLOYMENT_NAME,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Generate a structured JSONL dataset for model evaluation.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=4096\n",
        "        )\n",
        "\n",
        "        synthetic_text = response.choices[0].message.content\n",
        "        \n",
        "        synthetic_text = clean_json_response(synthetic_text)\n",
        "        synthetic_data.append(synthetic_text)\n",
        "    \n",
        "    joined_synthetic_data = []\n",
        "    joined_synthetic_data = '\\n'.join(synthetic_data)\n",
        "\n",
        "    print(joined_synthetic_data)\n",
        "    return joined_synthetic_data\n",
        "\n",
        "\n",
        "def save_to_jsonl(data, output_file):\n",
        "    \"\"\"Save synthetic data to a JSONL file.\"\"\"\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:        \n",
        "        f.write(data)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1739999762501
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute Synthetic Data Generation\n",
        "\n",
        "The following code snippet demonstrates how to extract text from a PDF file, generate synthetic data using Azure OpenAI, and save the data to a JSONL file.\n",
        "\n",
        "1. **File Paths**:\n",
        "   - `pdf_path`: Specifies the path to the PDF file from which text will be extracted. Change this to your PDF file path.\n",
        "   - `output_file`: Specifies the path to the output JSONL file where the synthetic data will be saved.\n",
        "\n",
        "2. **Extracting Text from PDF**:\n",
        "   - The `extract_text_from_pdf` function is called with `pdf_path` as the argument to extract text from the specified PDF file.\n",
        "\n",
        "3. **Generating Synthetic Data**:\n",
        "   - If text is successfully extracted from the PDF, the `generate_synthetic_data` function is called with the extracted text to generate synthetic data.\n",
        "\n",
        "4. **Saving Synthetic Data**:\n",
        "   - The generated synthetic data is saved to the specified `output_file` using the `save_to_jsonl` function.\n",
        "   - A message is printed to indicate that the synthetic dataset has been saved to the output file.\n",
        "\n",
        "5. **Handling No Text Extraction**:\n",
        "   - If no text is extracted from the PDF, a message is printed to indicate that no text was extracted.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"data/MSFT_cloud_architecture_contoso.pdf\"  # Change to your PDF file path\n",
        "output_file = \"synthetic_data.jsonl\"\n",
        "\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "if text:\n",
        "    synthetic_data = generate_synthetic_data(text)\n",
        "    save_to_jsonl(synthetic_data, output_file)\n",
        "    print(f\"Synthetic dataset saved to {output_file}\")\n",
        "else:\n",
        "    print(\"No text extracted from the PDF.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1739999776604
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}